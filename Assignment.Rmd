---
title: "Assignment 2 Spatial Epidemiology"
author: "Ferrara Lorenzo, Lucchini Marco"
date: "29-11-2022"
---

```{r include=FALSE}
knitr::opts_chunk$set( echo=F )
knitr::opts_chunk$set(out.width = '70%')
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(tidy =TRUE)
knitr::opts_chunk$set(message = FALSE)
```

```{r include=FALSE}
library(spdep)
library(sf)
library(classInt)
library(maptools)
library(R2WinBUGS)
library(foreign)
library(spatialreg)

# load("data.Rdata")
```

## Description:

### Data of the incidence of larynx cancer diagnosed during 10 years (1982-1991) in the districts of Mersey and West Lancashire in the north-east of England is in the file larynx-dates.odc. They have detected 876 cases in 144 electoral districts. The number of expected cases were calculated using internal standardization, based on the specific rates of sex and age in the zone of study (with the population of the census 1991). 

### 1) Perform an exploratory analysis to study the possible spatial correlation of the larynx incidence (Standardized Mobility Ratio, SMR) (considering neighbors those regions that share geographic limits and the matrix of weights as the matrix standardized by rows)

<!-- NELLA EXPLORATRY ANALISYS MAGAR DEVO DIRE IL NUMER DI REGIONI, IL NUMERO DI LINKS, IL NUMERO TOTALE DI VICINI, IL IPO DI LINK USATO E DI WEIGHTS USATO. -->
<!-- USA LA FUNZIONE SUMARY PE FARTI VENIRE DELLE IDEE -->
<!-- summary(oggetto xxnb) o un summary simile -->

```{r}
rm(list=ls())

data = list(

N = 144, 
            
O = c(5, 7, 0, 0, 8, 5, 1, 10, 3, 5, 
4, 7, 3, 6, 1, 5, 4, 6, 3, 3, 2, 2, 9, 8, 6, 10, 9, 15, 14, 12, 
11, 23, 12, 4, 27, 10, 10, 10, 13, 10, 7, 13, 16, 4, 9, 4, 13, 
4, 10, 12, 18, 13, 25, 5, 12, 6, 5, 2, 3, 6, 4, 4, 6, 1, 5, 9, 
7, 1, 5, 1, 4, 3, 3, 2, 3, 9, 5, 9, 19, 4, 12, 1, 2, 8, 6, 4, 
2, 4, 8, 4, 6, 7, 4, 10, 4, 7, 8, 12, 12, 8, 6, 2, 7, 7, 7, 4, 
10, 8, 3, 5, 6, 9, 3, 10, 9, 8, 9, 6, 3, 1, 0, 1, 1, 3, 1, 0, 
1, 0, 2, 5, 0, 0, 1, 0, 0, 1, 2, 4, 2, 2, 3, 0, 0, 1), 

E = c(2.53, 3.94, 3.09, 2.75, 3.81, 3.66, 4.05, 3.07, 3.42, 4.24, 3.9, 3.58, 
3.68, 3.19, 5.15, 3.75, 3.37, 4.52, 1.94, 3.47, 3.48, 3.32, 4.52, 
9.2, 10.37, 8.08, 6.66, 7.7, 9.79, 9.77, 10.17, 9.93, 9.53, 10.59, 
7.64, 7.03, 4.09, 7.66, 8.02, 6.78, 9.85, 7.98, 8.14, 4.07, 7.54, 
7.04, 7.99, 8.1, 5.69, 6.37, 8.2, 6.52, 4.68, 9.11, 11.28, 6.18, 
4.79, 4.98, 6.47, 5.43, 6.51, 4.44, 6.69, 5.55, 6.24, 5.27, 5.61, 
5.31, 6.92, 5.17, 5.13, 3.73, 6.18, 9.44, 7.3, 7.63, 11.66, 6.52, 
6.64, 10.64, 7.15, 8.76, 7.48, 6.06, 6.36, 7.98, 8.92, 9.07, 
6.2, 7.37, 6.68, 8.62, 8.05, 5.97, 8.65, 7.04, 8.91, 5.95, 7.5, 
9.11, 10.59, 7.64, 8.91, 8.72, 11.7, 10.69, 7.73, 8.52, 7.81, 
9.24, 8.25, 10.72, 10.33, 8.19, 10.9, 7, 9.98, 10.1, 2.33, 3.49, 
0.78, 3.59, 2.47, 4.1, 1.82, 0.71, 1.44, 1.76, 3.39, 2.19, 1.66, 
1.35, 2.27, 2.11, 1.05, 2.22, 3.24, 2.65, 3.15, 1.94, 3.13, 1.95, 
2.16, 1.76))

SMR = data$O/data$E

lunghezza = length(SMR)

debug_on=F
codaPkg_on=F
```

```{r}
shp.file <- read_sf(dsn = ".", layer = "NWEngland")
spatial.file <- sf::as_Spatial(shp.file)
spatial.file$geometry <- NULL #Eliminate this geographic information
```

```{r}
df <- as.data.frame(spatial.file) #Create a data.frame
NW.dataset <- sp::SpatialPolygonsDataFrame(spatial.file, data = df )
```

First we visualize the distribution of the data on an histogram
```{r}
hist(SMR, xlab="Standardized Mortality Rate (SMR)", main="")
```

```{r}
NW.dataset$SMR<-SMR
NW.dataset$NAME = NW.dataset$poly_id
```

```{r}
brks <- round(quantile(NW.dataset$SMR, probs=seq(0,1,0.2)), digits=2)
my.colours <- c("yellow", "orange2", "red3", "brown", "black")
```

and on the map of the regions: 
```{r warning=FALSE}
plot(NW.dataset, col=my.colours[findInterval(NW.dataset$SMR, brks,all.inside=TRUE)])
legend("right", legend=leglabs(brks),fill=my.colours, bty="n",cex=0.99)
title(main=paste("Standardized Mortality Rate (SMR) during 1982-1991"))
text(coordinates(NW.dataset),label=as.factor(NW.dataset$NAME),cex=0.4)
```

We notice that in the center of the district there's a cluster of regions with high values of SMR, which slowly lower as we get away from the center.

<!-- #ALTRE PALETTE -->
```{r}
# my.colours<-gray.colors(5,0.95,0.2)
# my.colours<- terrain.colors(5)
# my.colours<- rev(terrain.colors(5))
```

Now we'll analyse the lattice structure of the data: let's consider neighbors those regions that share geographic limits:
```{r}
xxnb <- poly2nb(NW.dataset)
plot(NW.dataset, border="grey")
plot(xxnb, coordinates(NW.dataset), add=TRUE, col="blue")
```

```{r}
# summary.nb(xxnb)
```

```{r}
# cards <- card(xxnb) #Cardinalities of neighbours lists
# 
# maxconts <- which(cards == max(cards))[1] #Select the first region
# NW.dataset$NAME[maxconts]
```

```{r}
# fg <- rep("grey", length(cards))
# fg[maxconts] <- "red"
# fg[xxnb[[maxconts]]] <- "green"
# plot(NW.dataset, col=fg)
# text(coordinates(NW.dataset), label=NW.dataset$NAME,cex=0.5)
# title(main="Region with largest number of contiguities")
```

After establishing the neighbors we compute the weights (weights standardized by row). The weights corresponding to the first four regions are:
```{r}
weight.object<-nb2listw(xxnb, glist=NULL, style="W", zero.policy=TRUE) 
weight.list = weight.object$weights

weight.list[1:4]
```

Now we can perform a Moran test to prove or disprove the presence of spatial autocorrelation:

- We do it first using Moran's I statistic:
```{r}
sids.moran<-moran.test(NW.dataset$SMR, weight.object) #Standardazed weights
sids.moran
```
The I statistic is 0.313, which is quite different from the expected value (-0.0069), indeed the p-value is very low ($p=2.4\cdot10^{-10}$), so we refuse the null hypothesis.

- We also do it through a Monte Carlo approach:
```{r}
set.seed(1234)
sids.moran.mc<-moran.mc(NW.dataset$SMR, listw=weight.object, nsim=9000)
sids.moran.mc
```
and the p-value is $p=0.000111$, therefore we refuse the null hypothesis.

Both methods lead us to the conclusion that in this phenomenon spatial correlation is present. 

```{r}
st.SMR<-(NW.dataset$SMR-mean(NW.dataset$SMR))/sd(NW.dataset$SMR)
xx<-moran.plot(st.SMR, weight.object ,labels=as.factor(NW.dataset$NAME), pch=19, main="Moran Plot")
```


```{r}
sids.local.moran<-localmoran(NW.dataset$SMR , weight.object)

dat<-data.frame(name = NW.dataset$NAME,sids.local.moran)
colnames(dat)[6]="P-value"
slot(NW.dataset,"data")<-cbind(slot(NW.dataset,"data"),sids.local.moran)

#Intervals
l.inf<-round(min(NW.dataset$Ii),digits=2)
l.sup<-round(max(NW.dataset$Ii),digits=2)

I.b5<-findInterval(NW.dataset$Ii,c((l.inf-0.01),-1,-0.5,0.5,1.5,(l.sup+0.01)))
breakI<-c((l.inf-0.1),-1,-0.5,0.5,1.5,(l.sup+0.1))
pal.I<- c("yellow3", "orange2", "gray80", "grey60","grey15")

# to look a list of colours of the R, use the function colours()

#Colour assignment based on intervals
plot(NW.dataset, col=pal.I[I.b5], main="Test statistic")
text(coordinates(NW.dataset),label=as.factor(NW.dataset$NAME),cex=0.4)
legend("bottomright", legend=leglabs(breakI),fill=pal.I, cex=0.4)#, bty="n")
```


```{r}
plot(NW.dataset, col=my.colours[findInterval(NW.dataset$SMR, brks,all.inside=TRUE)])
legend("right", legend=leglabs(brks),fill=my.colours, bty="n",cex=0.4)
title(main=paste("Standardized Mortality Rate (SMR) during 1982-1991"))
text(coordinates(NW.dataset),label=as.factor(NW.dataset$NAME),cex=0.4)
```


### 2) If we consider that the number of cases of Larynx are a Poisson distribution, are data overdispersed?

We try to fit the data with a Poisson model without random effects. 

```{R poisson 1}
laryx.Poisson<-paste(getwd(),"/laringe_poisson_model.odc",sep="")
laryx.inits<-list(
                list(alpha0=0),
                list(alpha0=-5e-01),
                list(alpha0=rnorm(1))
            )

path.winbugs<-paste("C:\\Users\\lofer\\WinBUGS14")
```

```{R poisson 2}
mybugsdir<-"C:\\Users\\lofer\\OneDrive\\Documenti\\GitHub\\Epidemiology_2nd_Assignment\\Poisson"

results.Poisson<-bugs(data=data, inits=laryx.inits,
                  # parameters.to.save = c("RR","alpha0"),
                  parameters.to.save = c("alpha0"),
                  model.file=laryx.Poisson, n.chains=3, n.iter=2000, n.burnin = 1000,
                  bugs.directory=path.winbugs, debug=debug_on, codaPkg=codaPkg_on, working.directory=mybugsdir)

print(results.Poisson)
```

AGGIUNGERE IMMAGINE PRESA DA WINBUGS DI TRACEPLLTS DELLA DEVIANCE




If the Poisson model fits the data reasonably, we would expect the residual deviance to be roughly equal to the
degrees of freedom (n-p).

```{r}
# model.2 <- glm(SMR ~ , offset = log(pop), family = poisson(link = "log"), data = data)
# summary(model.2)
```

â€¢ That the residual deviance is so large suggests that the
variation of the data exceeds the variation of a Poisson distributed variable, for which the variance equals the
mean.



### 3) Fit the SMR taking into account the over-dispersion of the data using a heterogeneity model, a spatial (CAR intrinsic) and the convolution model. Estimate the models using Bayesian inference (Gibbs Sampling). Use three chains of initial values.

- Heterogeneity model: unstructured overdispersion
```{R hetero}
mybugsdir<-"C:\\Users\\lofer\\OneDrive\\Documenti\\GitHub\\Epidemiology_2nd_Assignment\\Hetero"

laryx.heterogenity<-paste(getwd(),"/laryx-heterogeneity-model.odc",sep="")
laryx.inits.heterogenity<-list( list(tau.h = 1, alpha0 = 0, h= rep(0, lunghezza)),
                                list(tau.h = 0.005, alpha0 = -0.5, h= rep(0, lunghezza)), 
                                list(tau.h = 0.5, alpha0 = 1, h= rep(0, lunghezza)))

results.heterogenity<-bugs(data=data,inits=laryx.inits.heterogenity, 
                           parameters.to.save = c("alpha0","tau.h","sigma.h"),
                           model.file=laryx.heterogenity,n.chains=3,n.iter=10000,n.burnin=1000,n.thin = 10,
                           bugs.directory=path.winbugs,debug=debug_on,codaPkg=codaPkg_on,working.directory=mybugsdir)

print(results.heterogenity)

results.heterogenity<-bugs(data=data,inits=laryx.inits.heterogenity, 
                           parameters.to.save = c("alpha0","tau.h","sigma.h","resid","RR"),
                           model.file=laryx.heterogenity,n.chains=3,n.iter=10000,n.burnin=1000,n.thin = 10,
                           bugs.directory=path.winbugs,debug=debug_on,codaPkg=codaPkg_on,working.directory=mybugsdir)
```

(for the sake of clarity we didn't show the estimates of RR)
<!-- # RR = E_i exp( alpha0 + alpha1*X+s[i]) -->
<!-- # resid = exp(s[i]) -->
<!-- # se resid>1  vuol dir che s[i]>0 => il random effect aumenta il rischio -->
<!-- # al contrario se resid<1 => s[i]<0 => il random effect diminuisce il rischio -->

- Spatial model: we model a structured overdispersion, using the spatial structure we have created above (the neighbors, the weights and the corresponding adjacency matrix)
```{R spatial 1}
laryx.spatial<-paste(getwd(),"//laryx-spatial-model.odc",sep="")

w.laryx<-nb2listw(xxnb, style="W", zero.policy=TRUE)

adj<-unlist(w.laryx$neighbours)
adj<-adj[adj!=0]
data.spatial<-list(N=data$N,O=data$O,E=data$E,adj=adj,num=card(xxnb),sumNumNeigh=sum(card(xxnb)) )
```

```{R spatial 2}

mybugsdir<-"C:\\Users\\lofer\\OneDrive\\Documenti\\GitHub\\Epidemiology_2nd_Assignment\\Spatial"

laryx.inits.spatial<-list(
  list(alpha0=0,s=rnorm(lunghezza),tau.s=exp(rnorm(1))),
  list(alpha0=-5e-01,s=rnorm(lunghezza),tau.s=exp(rnorm(1))),
  list(alpha0=rnorm(1),s=rnorm(lunghezza),tau.s=exp(rnorm(1))))

results.spatial<-bugs(data=data.spatial,inits=laryx.inits.spatial,
                           parameters.to.save = c("alpha0","tau.s","sigma.s"),
                           model.file=laryx.spatial,n.chains=3,n.iter=7000,n.burnin = 1000,n.thin=8,
                           bugs.directory=path.winbugs,debug=debug_on,codaPkg=codaPkg_on,working.directory=mybugsdir)

print(results.spatial)

results.spatial<-bugs(data=data.spatial,inits=laryx.inits.spatial,
                           parameters.to.save = c("alpha0","tau.s","sigma.s", "RR", "resid"),
                           model.file=laryx.spatial,n.chains=3,n.iter=7000,n.burnin = 1000,n.thin=8,
                           bugs.directory=path.winbugs,debug=debug_on,codaPkg=codaPkg_on,working.directory=mybugsdir)
```

- Convolution model: we create a model which takes into account both a structured (spatial) overdispersion and an unstructured (heterogeneity) one
```{R convolutional}

mybugsdir<-"C:\\Users\\lofer\\OneDrive\\Documenti\\GitHub\\Epidemiology_2nd_Assignment\\Convo"

laryx.convolution<-paste(getwd(),"//laryx-convolution-model.odc",sep="")
laryx.inits.convo<-list(
  list(alpha0=0.00000E+00, h=rnorm(lunghezza),tau.h=exp(rnorm(1)), s=rnorm(lunghezza),tau.s=exp(rnorm(1))),
  list(alpha0=-5.00000E-01,h=rnorm(lunghezza),tau.h=exp(rnorm(1)),s=rnorm(lunghezza),tau.s=exp(rnorm(1))),
  list(alpha0=rnorm(1), h=rnorm(lunghezza),tau.h=exp(rnorm(1)),s=rnorm(lunghezza),tau.s=exp(rnorm(1))))

results.convo<-bugs(data=data.spatial,inits=laryx.inits.convo,
                    parameters.to.save = c("alpha0","tau.s","sigma.s","tau.h","sigma.h"),
                    model.file=laryx.convolution,n.chains=3,n.iter=10000,n.burnin = 1000,n.thin=8,
                    bugs.directory=path.winbugs,debug=debug_on,codaPkg=codaPkg_on,working.directory=mybugsdir)

print(results.convo)

results.convo<-bugs(data=data.spatial,inits=laryx.inits.convo,
                    parameters.to.save = c("RR", "alpha0","sigma.s","sigma.h", "frac.spatial","s2.marginal", "sigma2.h"),
                    model.file=laryx.convolution,n.chains=3,n.iter=10000,n.burnin = 1000,n.thin=8,
                    bugs.directory=path.winbugs,debug=debug_on,codaPkg=codaPkg_on,working.directory=mybugsdir)
```

### 4) Check the convergence and study the autocorrelation of the chain. Decide which model fit better the data.

To study the convergence we look both at the traceplots of the fitted parameters ( the 3 chains should stabilize after some time around the same value) and at the $\hat{R}$ coefficient of the parameters of the models (we'll assume that convergence is reached if $\hat{R} < 1.05$)

- Spatial:

All the $\hat{R}$ coefficients are below 1.05, indeed using "prod( results.spatial$summary[,8] < 1.05 )" we get
```{r}
prod( results.spatial$summary[,8] < 1.05 )
```
and the traceplots are:

![](./Immagine.png)

- Heterogeneity:

All the $\hat{R}$ coefficients are below 1.05, indeed using "prod( results.spatial$summary[,8] < 1.05 )" we get
```{r}
prod( results.heterogenity$summary[,8] < 1.05 )
```

and the traceplots are:

![](./Immagine.png)

- Convolutional:

All the $\hat{R}$ coefficients are below 1.05, indeed using "prod( results.spatial$summary[,8] < 1.05 )" we get
```{r}
prod( results.convo$summary[,8] < 1.05 )
```

and the traceplots are:

![](./Immagine.png)

Since there are more than one random effect, the convergence of the parameters is harder to reach because the Monte Carlo Markov Chain struggles to find stable values to converge to. 
So we'll avoid choosing the Convolutional model.

```{R}
df= data.frame( type=c("spatial", "heterogeneity", "convolutional"), 
                DIC = c(results.spatial$DIC, results.heterogenity$DIC, results.convo$DIC) )
df
```

The DIC index is an estimate of expected predictive error, so we'll choose the model with the lowest DIC. The Spatial model and the Convolutional model are almost comparable, but even though the Convolutional has a slightly lower DIC we'll choose the Spatial, due to the considerations we have made above.

### 5.1) Using the model that better fit the data: Make a plot of the observed SMRs and Relative Risk estimated by the model (mean posterior of SMR), using the percentiles of the values. Compare them and discuss the differences.

```{r}
chosen.model = results.spatial
RR.vector = chosen.model$mean$RR
```

```{r}
my.colours <- c("yellow", "orange2", "red3", "brown", "black")
brks <- round(quantile(NW.dataset$SMR, probs=seq(0,1,0.2)), digits=2)

layout(matrix(c(1,2,3), nrow = 1), widths = c(3,1.7,3))

plot(NW.dataset, col=my.colours[findInterval(SMR, brks,all.inside=TRUE)], main="SMR")
text(coordinates(NW.dataset), label=round(SMR, digits=2),cex=0.5)
# legend("bottom", legend=leglabs(names(brks)),fill=my.colours, bty="n",cex=0.83, title="Percentils")

plot.new()
legend("center", legend=leglabs(names(brks)),fill=my.colours, bty="n",cex=1, title="Percentils")

brks <- round(quantile(RR.vector, probs=seq(0,1,0.2)), digits=2)
plot(NW.dataset, col=my.colours[findInterval(RR.vector, brks,all.inside=TRUE)], main="RR")
text(coordinates(NW.dataset), label=round(RR.vector, digits=2),cex=0.5)
# legend("bottom", legend=leglabs(names(brks)),fill=my.colours, bty="n",cex=0.83, title="Percentils")
```


### 5.2) Make a plot showing the posterior probability of relative risk of the electoral districts that exceed 1. What do you observe? 

```{r}
RR.simul=chosen.model$sims.list$RR
prob.vector = rep(NA, lunghezza)
for (i in 1:lunghezza) {
  prob.vector[i]=sum(RR.simul[,i]>1) / (dim(RR.simul)[1])
}
PV = round(prob.vector, digits = 2)

brks <- round(quantile(PV, probs=seq(0,1,0.2)), digits=2)
plot(NW.dataset, col=my.colours[findInterval(PV, brks,all.inside=TRUE)], main="PV")
text(coordinates(NW.dataset), label=round(PV, digits=2),cex=0.5)
legend("left", legend=leglabs(brks),fill=my.colours, bty="n",cex=0.83, title="")
```

### 5.3) Make a plot showing the random effects for each districts (exponential scale) and another with the significance of this parameters. What do you observe?

The random effect is the spatial effect $s[i], \forall i\in\{1, .., 144\}$. If we want to see it in exponential scale, we'll use $resid[i]=e^{s[i]}$
#il random effect dovrebbe essere s[i]  o quello che Ã¨
#la significance Ã¨ il resid

```{r}
resid=chosen.model$mean$resid
random.effect = log(resid)

my.colours <- c("yellow", "orange2", "red3", "brown", "black")

layout(matrix(c(1,2,3), nrow = 1), widths = c(3,1.2,3))
# layout(matrix(c(1,2), nrow = 1))

brks <- round(quantile(random.effect, probs=seq(0,1,0.2)), digits=2)
plot(NW.dataset, col=my.colours[findInterval(random.effect, brks,all.inside=TRUE)], main="Random Effect")
text(coordinates(NW.dataset), label=round(random.effect, digits=2),cex=0.5)
legend( "bottom", legend=leglabs(brks),fill=my.colours, bty="n",cex=0.8)

plot.new()
# legend("bottom", legend=leglabs(names(brks)),fill=my.colours, bty="n",cex=0.99)

brks <- round(quantile(resid, probs=seq(0,1,0.2)), digits=2)
plot(NW.dataset, col=my.colours[findInterval(resid, brks,all.inside=TRUE)], main="resid")
text(coordinates(NW.dataset), label=round(resid, digits=2),cex=0.5)
legend("bottom", legend=leglabs(names(brks)),fill=my.colours, bty="n",cex=0.8)
```


```{r}
save.image("data.Rdata")
```